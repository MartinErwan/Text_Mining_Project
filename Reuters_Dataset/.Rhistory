entropy_overlap(clean_crp,CleanItemsSetFrequent)
CleanItemsSetFrequent
items(rules)
inspect(items(rules))
inspect(rules)
CleanItemsSetFrequent
#Document avant nettoyage
content(corpus_example[[1]])
#Nettoyage
cleaned_example = cleaning(corpus_example)
#Document après nettoyage
content(cleaned_example[[1]])
corpus_data= VCorpus(VectorSource(data$text))
dtm = DocumentTermMatrix(corpus_data)
dtm
dim(dtm)
#Fonction permettant de convertir un document en un sac de mot
toBagOfWords = function(document){
lw = words(document)
bag_of_words = list()
for(word in lw){
if(word %in% names(bag_of_words)){
bag_of_words[[word]] =  bag_of_words[[word]]+1
}
else{
bag_of_words[[word]] = 1
}
}
return(bag_of_words)
}
#Exemple d'application sur le premier document du corpus
d1 = clean_crp[[1]]
content(d1)
bw_d1 = toBagOfWords(d1)
bw_d1
#Exemple d'application sur le premier document du corpus
d1 = clean_crp[[1]]
content(d1)
nb_tot_documents= length(data_several_topics)
nb_tot_documents#2246 documents au total dans le training set
nb_tot_documents= length(data_several_topics)
nb_tot_documents#2246 documents au total dans le training set
data_several_topics = c(data_trade_training$text,data_acq_training$text,data_ship_training$text,data_silver_training$text,data_tea_training$text)
nb_tot_documents#2246 documents au total dans le training set
nb_tot_documents= length(data_several_topics)
nb_tot_documents#2246 documents au total dans le training set
#Probabilité de trade
nb_documents_classe_trade = length(data_trade_training$text) #369 documents de la classe trade
nb_documents_classe_trade
proba_trade  = nb_documents_classe_trade/nb_tot_documents
#Probabilité de acq
nb_documents_classe_acq = length(data_acq_training$text)
nb_documents_classe_acq #1650 documents de classe acq
proba_acq  = nb_documents_classe_acq/nb_tot_documents
#Probabilité de ship
nb_documents_classe_ship = length(data_ship_training$text)
nb_documents_classe_ship #197 documents de classe ship
proba_ship  = nb_documents_classe_ship/nb_tot_documents
#Probabilité de silver
nb_documents_classe_silver = length(data_silver_training$text)
nb_documents_classe_silver #21 documents de classe ship
proba_silver  = nb_documents_classe_silver/nb_tot_documents
#Probabilité de tea
nb_documents_classe_tea = length(data_tea_training$text)
nb_documents_classe_tea #9 documents de classe ship
proba_tea  = nb_documents_classe_tea/nb_tot_documents
#Trade
C1 =rowSums(TF[,1:nb_documents_classe_trade])
#ACQ
a = nb_documents_classe_trade+1
b = nb_documents_classe_trade+nb_documents_classe_acq
C2 =rowSums(TF[,a:b])
#Ship
a = b+1
b = a+nb_documents_classe_ship -1
C3 =rowSums(TF[,a:b])
#Silver
a = b+1
b = a+nb_documents_classe_silver -1
C4 =rowSums(TF[,a:b])
#tea
a = b+1
b = a+nb_documents_classe_tea -1
C5 =rowSums(TF[,a:b])
lambda = 1 #Un rajoute un coefficient lambda pour éviter les probabilités nulles
C = cbind(C1,C2,C3,C4,C5)+lambda #C contient l'occurence des mots dans chaque classe
View(C)
#MATRICE CONTENANT LES PROBABILITÉS DES MOTS SACHANT LA CLASSE : Proba_w_sachant_C
Sum_C = colSums(C)
Proba_W_sachant_C = cbind(C[,1]/Sum_C[1],C[,2]/Sum_C[2],C[,3]/Sum_C[3],C[,4]/Sum_C[4],C[,5]/Sum_C[5])
BayesianClassifier = function(d){
#On met le document sous forme de corpus
crp =  VCorpus(VectorSource(d$text))
#Nettoyage du document
clean_crp = cleaning(crp)
#On met le document sous forme de sac de mot
bow = toBagOfWords(clean_crp[[1]])
#On restreint le sac de mots aux mots présents connues grâce à l'apprentissage
r = Reduce(intersect,list(names(bow)),row.names(Proba_W_sachant_C))
#Nouveau sac de mot qui correspond au sac de mot restreint
new_bow = bow[r]
#Longueur du doc |d|
longueur_doc = as.bigz(Reduce('+',new_bow))
#Nombre de mots différents Nd
Nd = length(new_bow)
#Factoriel de la taille du doc |d|!
longueur_doc_facto = factorial(longueur_doc)
#Initilisation de liste contenant les probabilités d'appartenance à une classe sachant le document
proba = list(0,0,0,0,0)
for(i in 1:length(proba)){
for(k in names(new_bow)){
#Nk correspond à l'occurence du mot k dans le document
Nk = new_bow[[k]]
#Application de la formule générale sur les loi multinomiales
proba[[i]] = proba[[i]]+ log((Proba_W_sachant_C[k,i]**Nk)/factorial(Nk))
} #REMARQUE : On passe par les logarithmes car on manipule des valeurs très faibles qui sont arrondies à zéro par R sans les logarithmes
proba[[i]] = exp(mpfr((proba[[i]]+log(longueur_doc_facto)),precBits = 53)) #On récupère les vraies valeurs grâce à la librairie Rmpfr
}
#On divise les proba des documents sachant la classe par la somme totale des proba des documents sachant la classe pour éviter d'avoir à calculer la probabilité des documents
sum_proba = Reduce("+",proba)
for(i in 1:length(proba)){
proba[[i]] = proba[[i]]/sum_proba
}
#Traitement Post Process pour afficher proprement les différentes probabilités des classes sachant le document ainsi que le nom de la classe la plus probable
id = 1
classe = list("1"="trade","2"="acq","3"="ship","4"="silver","5"="tea")
for(i in 1:length(proba)){
print(paste("Le document a",toString(round(as.numeric(100*proba[[i]]),4)),"% de chance d'appartenir à la classe",toString(classe[[i]])))
if(proba[[i]]>proba[[id]]){
id = i
}
}
print(paste("Le topic le plus probable est le topic",toString(classe[[id]])))
}
nb_tot_documents= length(data_several_topics)
nb_tot_documents#2246 documents au total dans le training set
#Probabilité de trade
nb_documents_classe_trade = length(data_trade_training$text) #369 documents de la classe trade
nb_documents_classe_trade
proba_trade  = nb_documents_classe_trade/nb_tot_documents
lambda = 1 #Un rajoute un coefficient lambda pour éviter les probabilités nulles
C = cbind(C1,C2,C3,C4,C5)+lambda #C contient l'occurence des mots dans chaque classe
View(C)
#Importation des packages nécessaires
library("tm")
library("readtext")
library(wordcloud)
library(stopwords)
library(gmp)
library(Rmpfr)
library(arules)
library("stringr")
library("geometry")
#Work Directory
setwd("h:/Desktop/ING3/S1/Text mining/projet/Reuters_Dataset")
cleaning = function(corpus){
#Enlever les majuscules
corpus = tm_map(corpus, content_transformer(tolower))
#Enlever la ponctuation
corpus = tm_map(corpus,removePunctuation)
#Enlever les stopwords
stw = stopwords("english")
corpus = tm_map(corpus,removeWords,stopwords("english"))
corpus = tm_map(corpus, removeNumbers)
return(corpus)
}
example = readtext(file = "test/bop/0010454")
corpus_example = VCorpus(VectorSource(example$text))
#Document avant nettoyage
content(corpus_example[[1]])
#Nettoyage
cleaned_example = cleaning(corpus_example)
#Nettoyage
cleaned_example = cleaning(corpus_example)
#Document après nettoyage
content(cleaned_example[[1]])
#II.A) Show the high dimensionality and sparsity of the data.
data = readtext(file = "training/acq/*")
corpus_data= VCorpus(VectorSource(data$text))
dtm = DocumentTermMatrix(corpus_data)
dtm
dim(dtm)
M = as.matrix(dtm)
M[M>0] = 1
nb_coeff_non_nul = sum(M) # 118 252 coeff non nuls dans la matrice de mots du corpus
nb_coeff_non_nul
nb_coeff = length(M) #32 392 800  coeff dans la matrice de mots ==> prouve la grande dimensionnalité des données
nb_coeff
sparsity = (1 -nb_coeff_non_nul/nb_coeff)*100
sparsity
wc = function(data){
crp =  VCorpus(VectorSource(data$text))
clean_crp = cleaning(crp)
dtm = DocumentTermMatrix(clean_crp)
m = as.matrix(dtm)
m[m>0] = 1
frequency = colSums(m)
frequency = sort(frequency, decreasing = TRUE)
words = names(frequency)
wordcloud(words[1:50],frequency[1:50],scale=c(2,1))
}
wc = function(data){
crp =  VCorpus(VectorSource(data$text))
clean_crp = cleaning(crp)
dtm = DocumentTermMatrix(clean_crp)
m = as.matrix(dtm)
m[m>0] = 1
frequency = colSums(m)
frequency = sort(frequency, decreasing = TRUE)
words = names(frequency)
wordcloud(words[1:50],frequency[1:50],scale=c(2,1))
}
#Importation des documents du topic trade
data_trade_training = readtext(file = "training/trade/*")
data_trade_test = readtext(file="test/trade/*")
#Wordcloud du training
wc(data_trade_training)
#Wordcloud du test
wc(data_trade_test)
data_ship_training = readtext(file = "training/ship/*")
data_ship_test = readtext(file = "test/ship/*")
#Wordcloud du training
wc(data_ship_training)
#Wordcloud du test
wc(data_ship_test)
#Importation des topics (en plus de ceux déjà importés)
data_acq_training = readtext(file = "training/acq/*")
data_several_topics = c(data_trade_training$text,data_acq_training$text,data_ship_training$text,data_silver_training$text,data_tea_training$text)
#Organisation du corpus
# Documents 1 à 369 : Topic "Trade"
# Documents 370 à 2019 : Topic "acq"
# Documents 2020 à 2216 : Topic "ship"
# Documents 2217 à 2237 : Topic "silver"
# Documents 2238 à 2246 : Topic "tea"
crp =  VCorpus(VectorSource(data_several_topics))
clean_crp = cleaning(crp)
dtm = DocumentTermMatrix(clean_crp,control = list(wordLengths=c(1,Inf)))
TF = t(as.matrix(dtm))
TF = t(as.matrix(dtm))
nb_doc = ncol(TF) #On récupère le nombre de documents du corpus
nb_doc = ncol(TF) #On récupère le nombre de documents du corpus
#Création de la matrice binaire telle que le coefficient vaut 0 si le mot n'apparait pas dans le document et 1 sinon
presence_doc = as.matrix(dtm) # Il s'agit de la matrice de Term Frequency avant sa transposition
nb_doc = ncol(TF) #On récupère le nombre de documents du corpus
#Création de la matrice binaire telle que le coefficient vaut 0 si le mot n'apparait pas dans le document et 1 sinon
presence_doc = as.matrix(dtm) # Il s'agit de la matrice de Term Frequency avant sa transposition
presence_doc[presence_doc>0]=1
#Somme sur les colonnes pour avoir le nombre de documents qui contiennent le mot en question
frequency = colSums(presence_doc)
#IDF
idf = log(nb_doc/frequency,2) # idf associé au terme w du corpus
#Matrice TF_IDF
tf_idf = idf*TF
#Conversion du tf_idf en dataframe pour simplifier son utilisation
tf_idf = as.data.frame(tf_idf)
cos_similarity = function(v1,v2){
norm_d1 = sqrt(dot(v1,v1))
norm_d2 = sqrt(dot(v2,v2))
ps = dot(v1,v2)
return((ps)/(norm_d1*norm_d2))
}
#Conversion de la matrice tf_idf en objet matrice pour pouvoir faire des produits scalaires et autre calcul sur les colonnes
tf_idf_m = as.matrix(tf_idf)
#Récupération du vecteur tf_idf du document (ici le document numéro 1 du corpus)
v1 = tf_idf_m[,1]
similarity_example1 = cos_similarity(v1,v1)
similarity_example1 #On trouve 1
#Récupération des vecteurs tf_idf des documents à comparer
#Ici on va comparer le document 1 au document 2 ( qui appartienne à la même classe "trade" d'après la construction du corpus ( les 369 premiers sont de la classe trade )
v2 = tf_idf_m[,2]
similarity_example2 = cos_similarity(v1,v2)
similarity_example2 #On trouve 0.11
#Exemple 3 : Similarité d'un document avec un docuement appartenant à une classe différente
#Récupération du vecteur tf_idf du document (ici le document numéro 2000 du corpus donc un document de la classe acq)
v3 = tf_idf_m[,2000]
similarity_example3 = cos_similarity(v1,v3)
similarity_example3 #On trouve 0.0008==> valeur encore plus faible que si les documents appartenaient à la meme classe
#Fonction permettant de convertir un document en un sac de mot
toBagOfWords = function(document){
lw = words(document)
bag_of_words = list()
for(word in lw){
if(word %in% names(bag_of_words)){
bag_of_words[[word]] =  bag_of_words[[word]]+1
}
else{
bag_of_words[[word]] = 1
}
}
return(bag_of_words)
}
#Exemple d'application sur le premier document du corpus
d1 = clean_crp[[1]]
content(d1)
bw_d1 = toBagOfWords(d1)
bw_d1
nb_tot_documents= length(data_several_topics)
nb_tot_documents#2246 documents au total dans le training set
#Probabilité de trade
nb_documents_classe_trade = length(data_trade_training$text) #369 documents de la classe trade
nb_documents_classe_trade
proba_trade  = nb_documents_classe_trade/nb_tot_documents
#Probabilité de acq
nb_documents_classe_acq = length(data_acq_training$text)
nb_documents_classe_acq #1650 documents de classe acq
proba_acq  = nb_documents_classe_acq/nb_tot_documents
#Probabilité de ship
nb_documents_classe_ship = length(data_ship_training$text)
nb_documents_classe_ship #197 documents de classe ship
proba_ship  = nb_documents_classe_ship/nb_tot_documents
#Probabilité de silver
nb_documents_classe_silver = length(data_silver_training$text)
nb_documents_classe_silver #21 documents de classe ship
proba_silver  = nb_documents_classe_silver/nb_tot_documents
#Probabilité de tea
nb_documents_classe_tea = length(data_tea_training$text)
nb_documents_classe_tea #9 documents de classe ship
proba_tea  = nb_documents_classe_tea/nb_tot_documents
#Trade
C1 =rowSums(TF[,1:nb_documents_classe_trade])
#ACQ
a = nb_documents_classe_trade+1
b = nb_documents_classe_trade+nb_documents_classe_acq
C2 =rowSums(TF[,a:b])
#Ship
a = b+1
b = a+nb_documents_classe_ship -1
C3 =rowSums(TF[,a:b])
#Silver
a = b+1
b = a+nb_documents_classe_silver -1
C4 =rowSums(TF[,a:b])
#tea
a = b+1
b = a+nb_documents_classe_tea -1
C5 =rowSums(TF[,a:b])
lambda = 1 #Un rajoute un coefficient lambda pour éviter les probabilités nulles
C = cbind(C1,C2,C3,C4,C5)+lambda #C contient l'occurence des mots dans chaque classe
View(C)
#MATRICE CONTENANT LES PROBABILITÉS DES MOTS SACHANT LA CLASSE : Proba_w_sachant_C
Sum_C = colSums(C)
Proba_W_sachant_C = cbind(C[,1]/Sum_C[1],C[,2]/Sum_C[2],C[,3]/Sum_C[3],C[,4]/Sum_C[4],C[,5]/Sum_C[5])
BayesianClassifier = function(d){
#On met le document sous forme de corpus
crp =  VCorpus(VectorSource(d$text))
#Nettoyage du document
clean_crp = cleaning(crp)
#On met le document sous forme de sac de mot
bow = toBagOfWords(clean_crp[[1]])
#On restreint le sac de mots aux mots présents connues grâce à l'apprentissage
r = Reduce(intersect,list(names(bow)),row.names(Proba_W_sachant_C))
#Nouveau sac de mot qui correspond au sac de mot restreint
new_bow = bow[r]
#Longueur du doc |d| (ici sac de mot new_bow)
longueur_doc = as.bigz(Reduce('+',new_bow)) #as.bigz permet de manipuler par la suite des gros nombres
#Nombre de mots différents Nd
Nd = length(new_bow)
#Factoriel de la taille du doc |d|!
longueur_doc_facto = factorial(longueur_doc)
#Initilisation de liste contenant les probabilités d'appartenance à une classe sachant le document
proba = list(0,0,0,0,0)
for(i in 1:length(proba)){
for(k in names(new_bow)){
#Nk correspond à l'occurence du mot k dans le document
Nk = new_bow[[k]]
#Application de la formule générale sur les loi multinomiales
proba[[i]] = proba[[i]]+ log((Proba_W_sachant_C[k,i]**Nk)/factorial(Nk))
} #REMARQUE : On passe par les logarithmes pour eviter d'effectuer des produits nen les transformant en sommes
proba[[i]] = exp(mpfr((proba[[i]]+log(longueur_doc_facto)),precBits = 53)) #On récupère les vraies valeurs grâce à la librairie Rmpfr
}
#On divise les proba des documents sachant la classe par la somme totale des proba des documents sachant la classe pour éviter d'avoir à calculer la probabilité des documents
sum_proba = Reduce("+",proba)
for(i in 1:length(proba)){
proba[[i]] = proba[[i]]/sum_proba
}
#Traitement Post Process pour afficher proprement les différentes probabilités des classes sachant le document ainsi que le nom de la classe la plus probable
id = 1
classe = list("1"="trade","2"="acq","3"="ship","4"="silver","5"="tea")
for(i in 1:length(proba)){
print(paste("Le document a",toString(round(as.numeric(100*proba[[i]]),4)),"% de chance d'appartenir à la classe",toString(classe[[i]])))
if(proba[[i]]>proba[[id]]){
id = i
}
}
print(paste("Le topic le plus probable est le topic",toString(classe[[id]])))
}
#Traitement Post Process pour afficher proprement les différentes probabilités des classes sachant le document ainsi que le nom de la classe la plus probable
id = 1
classe = list("1"="trade","2"="acq","3"="ship","4"="silver","5"="tea")
for(i in 1:length(proba)){
print(paste("Le document a",toString(round(as.numeric(100*proba[[i]]),4)),"% de chance d'appartenir à la classe",toString(classe[[i]])))
if(proba[[i]]>proba[[id]]){
id = i
}
}
print(paste("Le topic le plus probable est le topic",toString(classe[[id]])))
BayesianClassifier = function(d){
#On met le document sous forme de corpus
crp =  VCorpus(VectorSource(d$text))
#Nettoyage du document
clean_crp = cleaning(crp)
#On met le document sous forme de sac de mot
bow = toBagOfWords(clean_crp[[1]])
#On restreint le sac de mots aux mots présents connues grâce à l'apprentissage
r = Reduce(intersect,list(names(bow)),row.names(Proba_W_sachant_C))
#Nouveau sac de mot qui correspond au sac de mot restreint
new_bow = bow[r]
#Longueur du doc |d| (ici sac de mot new_bow)
longueur_doc = as.bigz(Reduce('+',new_bow)) #as.bigz permet de manipuler par la suite des gros nombres
#Nombre de mots différents Nd
Nd = length(new_bow)
#Factoriel de la taille du doc |d|!
longueur_doc_facto = factorial(longueur_doc)
#Initilisation de liste contenant les probabilités d'appartenance à une classe sachant le document
proba = list(0,0,0,0,0)
for(i in 1:length(proba)){
for(k in names(new_bow)){
#Nk correspond à l'occurence du mot k dans le document
Nk = new_bow[[k]]
#Application de la formule générale sur les loi multinomiales
proba[[i]] = proba[[i]]+ log((Proba_W_sachant_C[k,i]**Nk)/factorial(Nk))
} #REMARQUE : On passe par les logarithmes pour eviter d'effectuer des produits nen les transformant en sommes
proba[[i]] = exp(mpfr((proba[[i]]+log(longueur_doc_facto)),precBits = 53)) #On récupère les vraies valeurs grâce à la librairie Rmpfr
}
#On divise les proba des documents sachant la classe par la somme totale des proba des documents sachant la classe pour éviter d'avoir à calculer la probabilité des documents
sum_proba = Reduce("+",proba)
for(i in 1:length(proba)){
proba[[i]] = proba[[i]]/sum_proba
}
#Traitement Post Process pour afficher proprement les différentes probabilités des classes sachant le document ainsi que le nom de la classe la plus probable
id = 1
classe = list("1"="trade","2"="acq","3"="ship","4"="silver","5"="tea")
for(i in 1:length(proba)){
print(paste("Le document a",toString(round(as.numeric(100*proba[[i]]),4)),"% de chance d'appartenir à la classe",toString(classe[[i]])))
if(proba[[i]]>proba[[id]]){
id = i
}
}
print(paste("Le topic le plus probable est le topic",toString(classe[[id]])))
}
#Application du classifieur bayésien
#Exemple 1
#Sur un document test ( qui n'a pas servi à la construction du classifieur ) dont on sait qu'il appartient à la classe acq
d1 = readtext(file = "test/acq/0009613")
BayesianClassifier(d1)
#Exemple 2
#Sur un document test ( qui n'a pas servi à la construction du classifieur ) dont on sait qu'il appartient à la classe trade
d2 = readtext(file = "test/trade/0009638")
BayesianClassifier(d2)
data_several_topics = c(data_ship_training$text[1:50],data_trade_training$text[1:50])
crp =  VCorpus(VectorSource(data_several_topics))
clean_crp = cleaning(crp)
trans = c()
for(k in 1:length(clean_crp)){
trans = c(trans,str_squish(gsub("[\r\n]", "",content(content(clean_crp)[[k]]))))
}
#Convertion des documents en objet "transaction"
transs <- as(strsplit(trans, " "), "transactions")
#Application de l'algorithme apriori qui va établir des règles à partir des transactions
rules = apriori(transs)
inspect(rules[1:10])
arules::itemFrequencyPlot(transs, topN = 20,
col = brewer.pal(8, 'Pastel2'),
main = 'Relative Item Frequency Plot',
type = "relative",
ylab = "Item Frequency (Relative)")
inspect(items(rules))
ItemsSetFrequent = inspect(items(rules))[[1]]#item set frequent On a donc 2491 item fréquent
ItemsSetFrequent
CleanItemsSetFrequent = unique(strsplit(gsub("[{} ]","",ItemsSetFrequent),",")) #On réecrit proprement les items set fréquents (en enlevant les doublons au passage ) pour pouvoir travailler dessus
CleanItemsSetFrequent
clustering = function(documents,itemsSet,overlap,itermax = 50) {
indice_doc_clusters = list() #list des clusters représentés par les numéros des documents les contenant
ind_doc = seq(1,length(documents)) #Indice de tous les documents
iter  = 0 #Nombre d'itération
a = 1 #Numéro du cluster crée
unused_ind = seq(1,length(documents))#Indice des documents pas utilisés
while((length(unused_ind)>0) & (length(itemsSet)>0) & iter<itermax){
#Recherche du meilleur item set au regard de la mesure d'overlap choisie
print(unused_ind)
ind_best_itemset = overlap(documents[unused_ind],itemsSet)
if(length(ind_best_itemset)>0){
print(itemsSet[ind_best_itemset])
indice_doc_cluster = c()
#Ajout dans le cluster des documents présentant l'item set sélectionné
for(j in 1:length(documents)){
document_words = words(documents[[j]])
if(all(itemsSet[[ind_best_itemset]]%in%document_words)){
indice_doc_cluster = c(indice_doc_cluster,j)
}
}
if(length(indice_doc_cluster)==0){
itemsSet = itemsSet[-ind_best_itemset]
}
else{
#Ajout du cluster choisi à la liste des clusters
indice_doc_clusters[toString(a)] = list(indice_doc_cluster)
unused_ind = unused_ind[!unused_ind%in%indice_doc_cluster]
itemsSet = itemsSet[-ind_best_itemset]
a = a+1
print(indice_doc_cluster)
#print(unused_ind)
iter = iter+1
}
}
else{
itemsSet = list()
}
}
return(indice_doc_clusters)
}
C_standard= clustering(clean_crp,CleanItemsSetFrequent,standard_overlap,500)
